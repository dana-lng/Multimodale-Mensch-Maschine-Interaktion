# üéì Multimodale Mensch-Maschine-Interaktion
---
## üë©‚Äçüíª Bearbeiterin
**Name:** Dana Lenzig  

---

## All Birds Must Fly: The Experience of Multimodal Hands-free Gaming with Gaze and Nonverbal Voice Synchronization

In dem Paper ‚ÄûAll Birds Must Fly: The Experience of Multimodal Hands-free Gaming with Gaze and Nonverbal Voice Synchronizatioy‚Äú untersuchten die Autorinnen und Autoren, ob Computerspiele vollst√§ndig freih√§ndig gesteuert werden k√∂nnen ‚Äì also nur durch Blickbewegungen und nichtverbale Stimmeingaben wie Summen. Ziel war es, eine intuitive und barrierefreie Steuerung zu entwickeln, die auch f√ºr Menschen mit k√∂rperlichen Einschr√§nkungen geeignet ist.

Dazu wurde ein eigenes 2D-Spiel entwickelt, in dem ein Pinguin durch Blickrichtung gesteuert und durch Summen zum Springen gebracht wird. In einer ersten Studie mit 15 Personen ohne Behinderung wurde diese Steuerung mit Maus und Tastatur verglichen, in einer zweiten Studie testeten 10 Personen mit motorischen oder sprachlichen Einschr√§nkungen die Methode.

Die Ergebnisse zeigten, dass die Blick- und Stimmeingabe zwar langsamer, aber fast genauso pr√§zise war wie die klassische Steuerung. Zudem wurde sie von den Teilnehmenden als spannender, unterhaltsamer und immersiver empfunden. Auch Personen mit Behinderungen konnten das Spiel erfolgreich bedienen.

Insgesamt zeigt die Studie, dass die Kombination aus Eye Tracking und nichtverbaler Stimmeingabe ein vielversprechender Ansatz f√ºr inklusive und freih√§ndige Interaktionen ist, der sich √ºber Spiele hinaus in weiteren digitalen Anwendungen einsetzen l√§sst. [1]

| <img src="img/SetUp.jpg" width="550"/> | <img src="img/Pinguin.jpg" width="200"/> |
|:-------------------------------------:|:-------------------------------------:|

Das Paper "Hands-Free Web Browsing: Enriching the User Experience with Gaze and Voice Modality" kombiniert Blicksteuerung und Spracheingabe, um Webseiten vollst√§ndig freih√§ndig bedienen zu k√∂nnen [Sengupta et al. 2018]. 
In einer Studie wurden typische Webaktionen wie Suchen, Scrollen und Link-Auswahl mit einem multimodalen Browser getestet, bei dem der Blick die Orientierung und die Sprache die Best√§tigung √ºbernimmt. Im Vergleich zur reinen Sprach- oder Blicksteuerung zeigte sich die kombinierte Methode als deutlich effizienter (z.‚ÄØB. 70‚ÄØ% schnellere Link-Auswahl). [2]

√Ñhnlich wie in ‚ÄûAll Birds Must Fly: The Experience of Multimodal Hands-free Gaming with Gaze and Nonverbal Voice Synchronizatioy‚Äú wird deutlich, dass die Kombination beider Eingaben die Schw√§chen der Einzelmodalit√§ten ausgleicht und ein fl√ºssigeres, barrierefreies Nutzungserlebnis schafft. Beide Arbeiten zeigen, dass multimodale Steuerung nicht nur technisch machbar, sondern auch subjektiv angenehmer und inklusiver ist.

**Folgende Tabelle vergleicht die beiden vorgeschlagenen Systeme:**

| Modalit√§t | Hedeshy et al. 2022 | Sengupta et al. 2018 |
| --- | --- | --- |
| Blick | Bewegung der Figur | Orientierung im Webbrowser |
| (non-) verbale Steuerung | Springen | Auswahl im Webbrowser |


---

## üß† AMITUDE-Modell
(A:) Ein 2D-Spiel namens ‚ÄúAll Birds Must Fly‚Äù, (U:) in dem vor allem Personen mit motorischen oder sprachlichen Einschr√§nkungen (T:) einen Pinguin durch Level steuern. (M:) Dies geschieht √ºber Blicksteuerung und nichtverbale Stimmeingabe (D:) mithilfe von Eye-Tracker und Mikrofon (E:) auf einem Laptop, (I:) wobei das System auf Eingaben reagiert und visuelles Feedback am Bildschirm gibt.

---

## üßÆ CARE-Modell
![Abbildung des CARE-Modells](img/CARE-Modell.png) 

---

## üß© CASE-Modell
Die Bewegungen des Pinguins werden CONCURRENT verarbeitet.

---

## üîó Literaturverzeichnis
[1] Ramin Hedeshy, Chandan Kumar, Mike Lauer, and Steffen Staab (2022).  
*All Birds Must Fly: The Experience of Multimodal Hands-free Gaming with Gaze and Nonverbal Voice Synchronization.*  
In: Proceedings of the 2022 International Conference on Multimodal Interaction (ICMI '22).  
[https://doi.org/10.1145/3536221.3556593](https://doi.org/10.1145/3536221.3556593)

[2] Korok Sengupta, Min Ke, Raphael Menges, Chandan Kumar, and Steffen Staab. 2018. Hands-free web browsing: enriching the user experience with gaze and voice modality. In Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications (ETRA '18). Association for Computing Machinery, New York, NY, USA, Article 88, 1‚Äì3. https://doi.org/10.1145/3204493.3208338
